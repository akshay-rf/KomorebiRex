import pandas as pd
import sqlite3
from surprise import Dataset, Reader
from sklearn.preprocessing import OrdinalEncoder, MinMaxScaler

from surprise.model_selection import cross_validate, GridSearchCV
from surprise import KNNBasic, KNNWithMeans, KNNWithZScore, KNNBaseline, SVD
from surprise import accuracy

connection = sqlite3.connect("demo.db")

# Load the dataset
query = pd.read_sql_query('SELECT * FROM superstore', connection)
df = pd.DataFrame(query).drop(['Row ID'], axis=1)


#<-- FINAL PREPROCESSING -->

for col in ['Profit', 'Sales']:
    upper_lt = df[col].mean() + 3*df[col].std()
    lower_lt = df[col].mean() - 3*df[col].std()
    df = df.loc[(df[col]<upper_lt) & (df[col]>lower_lt)]



def calculate_association_score(row):
    # Defining weights for each feature
    weights = {
       'Quantity ordered new': 0.2,
        'Unit Price': 0.5,
        'Sales': 0.4,       
        'Profit': 0.3,
        'returned': -0.9
    }
    
    # Extract relevant features and compute the association score
    score = (
        weights['Quantity ordered new'] * row['Quantity ordered new'] +
        weights['Unit Price'] * row['Unit Price'] + 
        weights['Sales'] * row['Sales'] +
        weights['Profit'] * row['Profit'] + 
        weights['returned'] * row['returned']
    )
    
    max_score = sum(weights.values())
    normalized_score = (score / max_score) * 10
    
    return normalized_score

def combine_relevant_features(df):
    # # Combine relevant text-based attributes into a single feature
    # df['combined_features'] = df['Product Name Encoded'].astype(str) + df['Product Category'] + ' ' + df['Product Sub-Category'] + '' + df['Customer Segment'] + df['Quantity ordered new'].astype(str) + df['Sales'].astype(str) + df['returned'].astype(str)

    df['score'] = df.apply(calculate_association_score, axis=1)
    print(df['score'].max())

    return df

def ordinal_encode(df, feature):
    encoder = OrdinalEncoder()
    df['Product Name Encoded'] = encoder.fit_transform(df[['Product Name']])

    scaler = MinMaxScaler(feature_range=(0, 1))  # Scale to range (0, 1)
    df[['Sales', 'Profit', 'Quantity ordered new', 'Unit Price']] = scaler.fit_transform(df[['Sales', 'Profit', 'Quantity ordered new', 'Unit Price']])
    print(df[df['returned']<0])
    return df

tranformed_df = ordinal_encode(df, 'Product Name')

relevant_features = ['Product Category', 'Product Sub-Category', 'Customer Segment', 'Sales', 
                     'Quantity ordered new', 'Discount', 'Profit', 'Unit Price', 'Shipping Cost', 
                     'returned']

# Group transactions by Customer ID and Product Name, aggregating relevant features
customer_product_grouped = tranformed_df.groupby(['Customer ID', 'Product Name Encoded']).agg({
    'Product Category': 'first',           # Product category for the customer-product pair
    'Product Sub-Category': 'first',       # Product sub-category for the customer-product pair
    'Customer Segment': 'first',           # Customer segment for the customer-product pair
    'Sales': 'sum',                        # Total sales amount for the customer-product pair
    'Quantity ordered new': 'sum',         # Total quantity ordered for the customer-product pair
    'Discount': 'mean',                    # Average discount applied to the product by the customer
    'Profit': 'sum',                       # Total profit generated by the customer-product pair
    'Unit Price': 'mean',                  # Average unit price of the product
    'Shipping Cost': 'mean',               # Average shipping cost of the product
    'returned': lambda x: (x == 1).sum()   # Count of returns for the customer-product pair
}).reset_index()



# <-- TRAINING AND TESTING THE MODEL -->

new_df = combine_relevant_features(customer_product_grouped)[['Customer ID', 'Product Name Encoded', 'score']]

reader = Reader(rating_scale=(1, 14))
data = Dataset.load_from_df(new_df[['Customer ID', 
                                  'Product Name Encoded', 
                                  'score']], reader)

raw_ratings = data.raw_ratings
import random
random.shuffle(raw_ratings)                 # shuffle dataset

threshold   = int(len(raw_ratings)*0.8)

train_raw_ratings = raw_ratings[:threshold] # 80% of data is trainset
test_raw_ratings  = raw_ratings[threshold:] # 20% of data is testset

data.raw_ratings = train_raw_ratings        # data is now the trainset
trainset         = data.build_full_trainset() 
testset          = data.construct_testset(test_raw_ratings)


models=[KNNBasic(),KNNWithMeans(),KNNWithZScore(),KNNBaseline(),SVD()] 
results = {}

for model in models:
    # perform 5 fold cross validation
    # evaluation metrics: mean absolute error & root mean square error
    CV_scores = cross_validate(model, data, measures=["MAE","RMSE"], cv=5, n_jobs=-1)  
    
    # storing the average score across the 5 fold cross validation for each model
    result = pd.DataFrame.from_dict(CV_scores).mean(axis=0).\
             rename({'test_mae':'MAE', 'test_rmse': 'RMSE'})
    results[str(model).split("algorithms.")[1].split("object ")[0]] = result

'''
Model Performance:
                                MAE      RMSE  fit_time  test_time
matrix_factorization.SVD   1.115877  2.190994  0.068479   0.005195
knns.KNNBaseline           1.165940  2.223608  0.112127   0.020733
knns.KNNBasic              1.296531  2.375148  0.127293   0.021100
knns.KNNWithZScore         1.468507  2.558213  0.165252   0.030498
knns.KNNWithMeans          1.507071  2.608480  0.090486   0.018983



    -KNNWithMeans has the least RMSE (root mean square error) among KNN algorithms
    -The model fit_time and test_time is the least for SVD

'''

performance_df = pd.DataFrame.from_dict(results)
print("Model Performance: \n")
print(performance_df.T.sort_values(by='RMSE'))


param_grid = {"n_factors": range(10,100,20),
              "n_epochs" : [5, 10, 20],
              "lr_all"   : [0.002, 0.005],
              "reg_all"  : [0.2, 0.5]}

gridsearchSVD = GridSearchCV(SVD, param_grid, measures=['mae', 'rmse'], cv=5, n_jobs=-1)
                                    
gridsearchSVD.fit(data)

print(f'MAE Best Parameters:  {gridsearchSVD.best_params["mae"]}')
print(f'MAE Best Score:       {gridsearchSVD.best_score["mae"]}\n')

print(f'RMSE Best Parameters: {gridsearchSVD.best_params["rmse"]}')
print(f'RMSE Best Score:      {gridsearchSVD.best_score["rmse"]}\n')

'''
MAE Best Parameters:  {'n_factors': 50, 'n_epochs': 20, 'lr_all': 0.005, 'reg_all': 0.2}
MAE Best Score:       1.1192705730012402

RMSE Best Parameters: {'n_factors': 10, 'n_epochs': 20, 'lr_all': 0.005, 'reg_all': 0.2}
RMSE Best Score:      2.1870825305144557

    -Post Hyperparameter Tuning with GridSearchCV, the best parameters are found to be different for MAE & RMSE metrics
    -'n_factors':50, 'n_epochs':10, 'lr_all':0.005 & 'reg_all': 0.2 have been chosen for building recommendations

'''

final_model = SVD(n_factors=50, n_epochs=10, lr_all=0.005, reg_all= 0.2)

# Fitting the model on trainset & predicting on testset, printing test accuracy
pred = final_model.fit(trainset).test(testset)

print(f'\nUnbiased Testing Performance')
print(f'MAE: {accuracy.mae(pred)}, RMSE: {accuracy.rmse(pred)}')

'''
Unbiased Testing Performance
MAE:  1.2182
RMSE: 2.4628
MAE: 1.2181591303900143, RMSE: 2.4628053726737846
    
    -The MAE & RMSE metrics for testset are comparable with what was obtained using cross validation & hyperparameter tuning stages with trainset. 
    -Chosen model hence again, generalizes well.

'''

def generate_recommendationsSVD(userID=13552, get_recommend =10):
    
    model = SVD(n_factors=50, n_epochs=10, lr_all=0.005, reg_all= 0.2)
    model.fit(trainset)
    
    with open(model_pkl_file, "wb") as file:
        pickle.dump(model, file)
        print("written")
    # predict rating for all pairs of users & items that are not in the trainset
    
    testset = trainset.build_anti_testset()
    predictions = model.test(testset)
    predictions_df = pd.DataFrame(predictions)
    
    # get the top get_recommend predictions for userID
    
    predictions_userID = predictions_df[predictions_df['uid'] == userID].\
                         sort_values(by="est", ascending = False).head(get_recommend)
    
    recommendations = []
    recommendations.append(list(predictions_userID['iid']))
    recommendations = recommendations[0]
    predicted_product_names = [encoded_to_name_map[encoded_value] for encoded_value in recommendations]

    
    return(predicted_product_names)

model_pkl_file = "userbased.pkl"  

encoded_to_name_map = dict(zip(df['Product Name Encoded'], df['Product Name']))
print(generate_recommendationsSVD(5, 10))





