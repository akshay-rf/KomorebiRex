import streamlit as st
from streamlit_option_menu import option_menu

from surprise import Dataset, Reader, SVD
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import linear_kernel
from sklearn.preprocessing import OrdinalEncoder, MinMaxScaler

import pandas as pd
import sqlite3

#<--WEB APP CONFIG-->
page_title = "KomorebiRex"
page_icon = ":pouch:"
layout = "centered"
st.set_page_config(page_title=page_title, page_icon=page_icon, layout=layout)


#<--CONNECTING TO SQLITE DB-->
connection = sqlite3.connect("demo.db")
query = pd.read_sql_query('SELECT * FROM superstore', connection)
df = pd.DataFrame(query).drop(['Row ID'], axis=1)

#<--FINAL PRE-PROCESSING FOR CONTENT-BASED FILTERING-->
customer_product_grouped = df.groupby(['Product Name']).agg({
    'Product Category': 'first',           # Product category for the customer-product pair
    'Product Sub-Category': 'first',       # Product sub-category for the customer-product pair
    'Product Container': 'first',          # Product packaging
    'Product Base Margin': 'mean',         # Product base margin for the product
    'Sales': 'sum',                        # Total sales amount for the customer-product pair
    'Quantity ordered new': 'sum',         # Total quantity ordered for the customer-product pair
    'Discount': 'mean',                    # Average discount applied to the product by the customer
    'Profit': 'sum',                       # Total profit generated by the customer-product pair
    'Unit Price': 'mean',                  # Average unit price of the product
    'Shipping Cost': 'mean',               # Average shipping cost of the product
    'returned': lambda x: (x == 1).sum()   # Count of returns for the customer-product pair
}).reset_index()

content_df = customer_product_grouped[['Product Name', 'Product Category', 
                   'Product Sub-Category', 'Product Container', 'Product Base Margin']]
content_df['Content'] = content_df.apply(lambda row: ' '.join(row.dropna().astype(str)), axis=1)

#
tfidf_vectorizer = TfidfVectorizer()
content_matrix = tfidf_vectorizer.fit_transform(content_df['Content'])

content_similarity = linear_kernel(content_matrix, content_matrix)

for col in ['Profit', 'Sales']:
    upper_lt = df[col].mean() + 3*df[col].std()
    lower_lt = df[col].mean() - 3*df[col].std()
    df = df.loc[(df[col]<upper_lt) & (df[col]>lower_lt)]



def calculate_association_score(row):
    # Defining weights for each feature
    weights = {
       'Quantity ordered new': 0.2,
        'Unit Price': 0.5,
        'Sales': 0.4,       
        'Profit': 0.3,
        'returned': -0.9
    }
    
    # Extract relevant features and compute the association score
    score = (
        weights['Quantity ordered new'] * row['Quantity ordered new'] +
        weights['Unit Price'] * row['Unit Price'] + 
        weights['Sales'] * row['Sales'] +
        weights['Profit'] * row['Profit'] + 
        weights['returned'] * row['returned']
    )
    
    max_score = sum(weights.values())
    normalized_score = (score / max_score) * 10
    
    return normalized_score

def combine_relevant_features(df):
    # # Combine relevant text-based attributes into a single feature
    # df['combined_features'] = df['Product Name Encoded'].astype(str) + df['Product Category'] + ' ' + df['Product Sub-Category'] + '' + df['Customer Segment'] + df['Quantity ordered new'].astype(str) + df['Sales'].astype(str) + df['returned'].astype(str)

    df['score'] = df.apply(calculate_association_score, axis=1)

    return df

def ordinal_encode(df, feature):
    #Ordinal Encoding on Product Name
    encoder = OrdinalEncoder()
    df['Product Name Encoded'] = encoder.fit_transform(df[['Product Name']])

    scaler = MinMaxScaler(feature_range=(0, 1))  # Scale to range (0, 1)
    df[['Sales', 'Profit', 'Quantity ordered new', 'Unit Price']] = scaler.fit_transform(df[['Sales', 'Profit', 'Quantity ordered new', 'Unit Price']])
    return df

def get_content_based_recommendations(product_name, top_n):
    index = content_df[content_df['Product Name'] == product_name].index[0]
    similarity_scores = content_similarity[index]
    similar_indices = similarity_scores.argsort()[::-1][1:top_n + 1]
    recommendations = content_df.loc[similar_indices, 'Product Name'].values
    return list(recommendations)

def generate_recommendationsSVD(userID, get_recommend):    
    # get the top get_recommend predictions for userID
    predictions_userID = predictions_df[predictions_df['uid'] == userID].sort_values(by="est", ascending = False).head(get_recommend)
    recommendations = []
    recommendations.append(list(predictions_userID['iid']))
    recommendations = recommendations[0]
    predicted_product_names = [encoded_to_name_map[encoded_value] for encoded_value in recommendations]

    
    return predicted_product_names

def get_hybrid_recommendations(user_id, product_id, top_n):
    content_based_recommendations = get_content_based_recommendations(product_id, top_n)
    collaborative_filtering_recommendations = generate_recommendationsSVD(user_id, top_n)
    hybrid_recommendations = list(set(content_based_recommendations + collaborative_filtering_recommendations))
    return hybrid_recommendations[:top_n]

def get_key_from_value(dct, value):
    reverse_dict = {v: k for k, v in dct.items()}
    return reverse_dict.get(value)

tranformed_df = ordinal_encode(df, 'Product Name')
encoded_to_name_map = dict(zip(df['Product Name Encoded'], df['Product Name']))

@st.cache_resource #caching the model and prediction dataframe
def get_trained_model(dff):
    new_df = combine_relevant_features(dff)[['Customer ID', 'Product Name Encoded', 'score']]

    reader = Reader(rating_scale=(1, 14))
    data = Dataset.load_from_df(new_df[['Customer ID', 
                                    'Product Name Encoded', 
                                    'score']], reader)

    raw_ratings = data.raw_ratings
    import random
    random.shuffle(raw_ratings)                 # shuffle dataset

    threshold   = int(len(raw_ratings)*0.8)

    train_raw_ratings = raw_ratings[:threshold] # 80% of data is trainset
    test_raw_ratings  = raw_ratings[threshold:] # 20% of data is testset

    data.raw_ratings = train_raw_ratings        # data is now the trainset
    trainset         = data.build_full_trainset() 
    testset          = data.construct_testset(test_raw_ratings)

    model = SVD(n_factors=50, n_epochs=10, lr_all=0.005, reg_all= 0.2)
    model.fit(trainset)

    testset = trainset.build_anti_testset()
    predictions = model.test(testset)
    predictions_df = pd.DataFrame(predictions)
    exceptions = dict()

    return model, predictions_df, exceptions



# Group transactions by Customer ID and Product Name, aggregating relevant features
customer_product_grouped = tranformed_df.groupby(['Customer ID', 'Product Name Encoded']).agg({
    'Product Category': 'first',           # Product category for the customer-product pair
    'Product Sub-Category': 'first',       # Product sub-category for the customer-product pair
    'Customer Segment': 'first',           # Customer segment for the customer-product pair
    'Sales': 'sum',                        # Total sales amount for the customer-product pair
    'Quantity ordered new': 'sum',         # Total quantity ordered for the customer-product pair
    'Discount': 'mean',                    # Average discount applied to the product by the customer
    'Profit': 'sum',                       # Total profit generated by the customer-product pair
    'Unit Price': 'mean',                  # Average unit price of the product
    'Shipping Cost': 'mean',               # Average shipping cost of the product
    'returned': lambda x: (x == 1).sum()   # Count of returns for the customer-product pair
}).reset_index()

model, predictions_df, exceptions = get_trained_model(customer_product_grouped)



# <--INTERFACE-->
st.markdown("<h1 style='text-align: center; color: white;'>KomorebiRex ðŸŒ„</h1>", unsafe_allow_html=True)

hide_st_style = """<style>
                #MainMenu {visibility : hidden;}
                footer {visibility : hidden;}
                header {visibility : hidden;}
                </style>
                """
st.markdown(hide_st_style, unsafe_allow_html=True)

nav_menu = option_menu(
    menu_title = None,
    options = ["User Based", "Product Based", "Hybrid"],
    icons = ["list-task", "folder"],
    orientation = "horizontal"
)


@st.experimental_dialog("Lack of User Interaction")
def popup_err(subtext):
    st.write(subtext)

@st.experimental_dialog("Feedback Updated")
def popup_feedback(subtext):
    st.write(subtext)


# <--USER BASED FILTERING INTERFACE-->
if nav_menu == "User Based":

    users_df = pd.read_sql_query('SELECT DISTINCT [Customer ID], [Customer Name] FROM superstore', connection)
    users = []

    for _, user in users_df.iterrows():
        users.append(f"{user['Customer ID']}-{user['Customer Name']}")

    selected_users = st.selectbox("Type or Select a user", users)

    col1,col2,col3 = st.columns(3)   
    
    if col2.button('Run User-Based Filtering', type='primary'):
        selected_users_id = selected_users.split("-")[0]
        count=0
        products_rec = generate_recommendationsSVD(int(selected_users_id), 30)
        if(len(products_rec)>0):
            products_rec.remove(products_rec[0])
        else:
            popup_err("User does not have enough records in the database to do collaborative filtering. This is called the cold-start problem and can be fixed by implementing a hybrid filtering system.")

        text = """
        ### Recommendations
        """
        for i in products_rec:
            if count==0:
                text+='\r- '+i+'\n'
            else:
                text+='- '+i+'\n'
            count+=1
            
        st.markdown(text)
      
#<--PRODUCT BASED FILTERING INTERFACE-->
if nav_menu == "Product Based":  
    products_df = pd.read_sql_query('SELECT DISTINCT [Product Name] FROM superstore', connection)
    products = [] 

    for _, product in products_df.iterrows():
        products.append(product['Product Name'])

    select_product = st.selectbox("Type or Select a product", products)

    col1,col2,col3 = st.columns(3)   
    if col2.button('Run Content-Based Filtering', type='primary'):
        products_rec = get_content_based_recommendations(select_product, 30)
        text = """
        ### Recommendations
        """
        count=0
        for i in products_rec:
            if count==0:
                text+='\r- '+i+'\n'
            else:
                text+='- '+i+'\n'
            count+=1
            
        st.markdown(text)

#<--HYBRID FILTERING INTERFACE-->
if nav_menu == "Hybrid":
    col1, col2 = st.columns(2)
    users_df = pd.read_sql_query('SELECT DISTINCT [Customer ID], [Customer Name] FROM superstore', connection)
    users = []

    for _, user in users_df.iterrows():
        users.append(f"{user['Customer ID']}-{user['Customer Name']}")

    with col1:
        selected_users = st.selectbox("Type or Select a user", users)   

    products_df = pd.read_sql_query('SELECT DISTINCT [Product Name] FROM superstore', connection)
    products = [] 

    for _, product in products_df.iterrows():
        products.append(product['Product Name'])

    with col2:
        select_product = st.selectbox("Type or Select a product", products)

    def on_button_click(button):
        popup_feedback("We won't be recommending you this item anymore.")
        st.session_state.last_clicked = button

    coll1,coll2,coll3 = st.columns(3)   
    
    if coll2.button('Run Hybrid Filtering', type='primary'):
        selected_users_id = selected_users.split("-")[0]
        selected_product_id = select_product

        products_rec = get_hybrid_recommendations(int(selected_users_id),selected_product_id, 30)

        text = """
        ### Recommendations
        """
        st.markdown(text)

        count=0
        if "last_clicked" in  st.session_state:
            # print(predictions_df)
            if(count==0):
                # print(encoded_to_name_map.values())
                if selected_users_id in exceptions.keys():
                    exceptions[selected_users_id].append(st.session_state.last_clicked)
                else:
                    exceptions[selected_users_id] = [st.session_state.last_clicked]
                print("exception added")


        for col in products_rec:
            try:
                if col not in exceptions[selected_users_id]:
                    st.button(col, on_click=on_button_click, kwargs={"button": col})
                    if "last_clicked" in  st.session_state:
                        count+=1
                        # print(predictions_df)
                        if(count==1):
                            # print(encoded_to_name_map.values())
                            if selected_users_id in exceptions.keys():
                                exceptions[selected_users_id].append(st.session_state.last_clicked)
                            else:
                                exceptions[selected_users_id] = [st.session_state.last_clicked]
                            print("exception added")
            except Exception:
                st.button(col, on_click=on_button_click, kwargs={"button": col})
                if "last_clicked" in  st.session_state:
                    count+=1
                    # print(predictions_df)
                    if(count==1):
                        # print(encoded_to_name_map.values())
                        if selected_users_id in exceptions.keys():
                            exceptions[selected_users_id].append(st.session_state.last_clicked)
                        else:
                            exceptions[selected_users_id] = [st.session_state.last_clicked]





            

        